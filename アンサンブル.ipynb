{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849f60a2-f160-44c7-aa1a-90a3f20277ae",
   "metadata": {},
   "source": [
    "# アンサンブル学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c4890-c810-4118-97b9-11e8deadb2d0",
   "metadata": {},
   "source": [
    "- ブレンディング\n",
    "- バギング\n",
    "- スタッキング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991ef84a-6f44-4815-b3bd-d0e6192fd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なモジュールをインポート\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings #ワーニング関連のモジュール？\n",
    "warnings.filterwarnings('ignore') #ワーニングが消える？\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import scipy.stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce9fd5c-9266-49bf-a366-e27800ae007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_1.csv\")\n",
    "\n",
    "# 目的変数\n",
    "y = df.loc[:,[\"SalePrice\"]]\n",
    "\n",
    "#説明変数\n",
    "X   = df.loc[:,[\"GrLivArea\",\"YearBuilt\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80dc077b-3af2-4b88-b923-a3213153cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "svm = SVR()\n",
    "dtr = DecisionTreeRegressor()\n",
    "rfr = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b058e4a-a3fd-4f03-adba-3dfc1f4ed392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングデータ分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e4491-2597-4eeb-96c3-7a193ff83b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "443aaf9e-52d7-485d-8388-af1530f1352b",
   "metadata": {},
   "source": [
    "## 【問題1】ブレンディングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387acfe-770c-48ea-9630-62698f4e521d",
   "metadata": {},
   "source": [
    "### ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証データに対する平均二乗誤差（MSE）が小さくなることを指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f35c5-6f38-421e-a41a-20a64011061b",
   "metadata": {},
   "source": [
    "## ブレンディングとは"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77cfbab-e6ce-4797-84d9-926d57834ce8",
   "metadata": {},
   "source": [
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3680e-63bb-42a9-a545-db8f6fac970c",
   "metadata": {},
   "source": [
    "- 手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "- ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "- 入力データの前処理の仕方（例：標準化、対数変換、PCAなど）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41047571-2863-496d-bbb1-e3f49a4fe583",
   "metadata": {},
   "source": [
    "重要なのはそれぞれのモデルが大きく異なることです。\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077103a-91f5-4485-bb0c-86c1a2a85573",
   "metadata": {},
   "source": [
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7edd818-aa18-4ae8-b77b-c3e9bb7aafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ブレンディング\"\"\"\n",
    "class scratch_brend():\n",
    "\n",
    "    def train(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"------------\")\n",
    "        print(\"デフォルトのMSE\")\n",
    "        print(\"------------\")\n",
    "        models = [lr,svm,dtr,rfr]\n",
    "        model_names =[\"Logistic Regression\",\"SVM\",\"Decision Tree\",\"Random Forest\"]\n",
    "        ans_mean = 0\n",
    "        count = 0\n",
    "        for model,model_name in zip(models,model_names):\n",
    "            print(model_name)\n",
    "            model.fit(X_train,y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            model_mean = mean_squared_error(y_test,y_pred)\n",
    "            print(model_mean)\n",
    "            print(\"------------\")\n",
    "            ans_mean += model_mean\n",
    "            count += 1\n",
    "        ans = ans_mean/count\n",
    "        print(\"ブレンディング後のMSE\")\n",
    "        return ans\n",
    "\n",
    "    def trans(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"------------\")\n",
    "        print(\"標準化したMSE\")\n",
    "        print(\"------------\")\n",
    "        X_train_trans = scipy.stats.zscore(X_train)\n",
    "        X_test_trans = scipy.stats.zscore(X_test)\n",
    "        models = [lr,svm,dtr,rfr]\n",
    "        model_names =[\"Logistic Regression\",\"SVM\",\"Decision Tree\",\"Random Forest\"]\n",
    "        ans_mean = 0\n",
    "        ans_mean = 0\n",
    "        count = 0\n",
    "        for model,model_name in zip(models,model_names):\n",
    "            print(model_name)\n",
    "            model.fit(X_train_trans,y_train)\n",
    "            y_pred = model.predict(X_test_trans)\n",
    "            model_mean = mean_squared_error(y_test,y_pred)\n",
    "            print(model_mean)\n",
    "            print(\"------------\")\n",
    "            ans_mean += model_mean\n",
    "            count += 1\n",
    "        ans = ans_mean/count\n",
    "        print(\"ブレンディング後のMSE\")\n",
    "        return ans\n",
    "\n",
    "    def log(self,X_train,y_train,X_test,y_test):\n",
    "        print(\"------------\")\n",
    "        print(\"対数変換したMSE\")\n",
    "        print(\"------------\")\n",
    "        X_train_log = X_train.apply(np.log)\n",
    "        X_test_log = X_test.apply(np.log)\n",
    "        models = [lr,svm,dtr,rfr]\n",
    "        model_names =[\"Logistic Regression\",\"SVM\",\"Decision Tree\",\"Random Forest\"]\n",
    "        ans_mean = 0\n",
    "        ans_mean = 0\n",
    "        count = 0\n",
    "        for model,model_name in zip(models,model_names):\n",
    "            print(model_name)\n",
    "            model.fit(X_train_log,y_train)\n",
    "            y_pred = model.predict(X_test_log)\n",
    "            model_mean = mean_squared_error(y_test,y_pred)\n",
    "            print(model_mean)\n",
    "            print(\"------------\")\n",
    "            ans_mean += model_mean\n",
    "            count += 1\n",
    "        ans = ans_mean/count\n",
    "        print(\"ブレンディング後のMSE\")\n",
    "        return ans\n",
    "\n",
    "scb = scratch_brend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7bcadc-178c-4184-b561-8aa58f655fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "デフォルトのMSE\n",
      "------------\n",
      "Logistic Regression\n",
      "1580858237.456167\n",
      "------------\n",
      "SVM\n",
      "5175145038.668915\n",
      "------------\n",
      "Decision Tree\n",
      "2042664715.5924656\n",
      "------------\n",
      "Random Forest\n",
      "1451597763.1440623\n",
      "------------\n",
      "ブレンディング後のMSE\n",
      "2562566438.7154026\n"
     ]
    }
   ],
   "source": [
    "print(scb.train(X_train,y_train,X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91171abe-f888-469d-a9bd-2283c4c7851c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "標準化したMSE\n",
      "------------\n",
      "Logistic Regression\n",
      "1581582628.6818948\n",
      "------------\n",
      "SVM\n",
      "5176763974.21277\n",
      "------------\n",
      "Decision Tree\n",
      "3375402398.8912673\n",
      "------------\n",
      "Random Forest\n",
      "1683411954.0409417\n",
      "------------\n",
      "ブレンディング後のMSE\n",
      "2954290238.9567184\n"
     ]
    }
   ],
   "source": [
    "print(scb.trans(X_train,y_train,X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21f91251-99a1-4795-87ea-55e8beaaf743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "対数変換したMSE\n",
      "------------\n",
      "Logistic Regression\n",
      "1871291769.8489769\n",
      "------------\n",
      "SVM\n",
      "5174748361.732942\n",
      "------------\n",
      "Decision Tree\n",
      "2100443735.6541095\n",
      "------------\n",
      "Random Forest\n",
      "1419750500.0100358\n",
      "------------\n",
      "ブレンディング後のMSE\n",
      "2641558591.8115163\n"
     ]
    }
   ],
   "source": [
    "print(scb.log(X_train,y_train,X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5acfd8-0574-48fb-8877-601895badd29",
   "metadata": {},
   "source": [
    "## 【問題2】バギングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e4e7f-7344-467b-a168-e1812c2d4eae",
   "metadata": {},
   "source": [
    "### バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4a424-e673-47b6-a734-67299c04265b",
   "metadata": {},
   "source": [
    "## バギングとは"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491f5c8-4159-4ee5-80b5-b3ecbd8503dd",
   "metadata": {},
   "source": [
    "バギングは入力データの選び方を多様化する方法です。訓練データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c30ee30-1d09-4006-95e2-b2ab227254db",
   "metadata": {},
   "source": [
    "推定結果の平均をとる部分はブレンディングと同様の実装になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8d5c09-e7f3-409a-b742-5e20192ebcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"バギング\"\"\"\n",
    "\n",
    "class scratch_bagi():\n",
    "    def bagi(self,X,y,X_test,y_test):\n",
    "        print(\"------------\")\n",
    "        print(\"バギング\")\n",
    "        #print(\"------------\")\n",
    "        # Xのインデックス分の配列を作る（インデックス番号を指定するため）\n",
    "        X_np = np.arange(X.shape[0])\n",
    "        # 最終的なMSEを保管するための変数\n",
    "        fainal_ans =0\n",
    "        fainal_count = 0\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # サンプルの配列の数分だけ繰り返す\n",
    "        for i in range(5):\n",
    "            # 重複ありのtrainデータを作る\n",
    "            X_train=np.array([])\n",
    "            y_train=np.array([])\n",
    "            select = np.random.choice(X_np, size=int(X.shape[0]*0.8))\n",
    "            for j in range(len(select)):\n",
    "                X_train = np.append(X_train,X[int(select[j])])\n",
    "                y_train = np.append(y_train,y[int(select[j])])\n",
    "            X_train = X_train.reshape(-1,2)\n",
    "\n",
    "            models = [lr,svm,dtr,rfr]\n",
    "            model_names =[\"Logistic Regression\",\"SVM\",\"Decision Tree\",\"Random Forest\"]\n",
    "            ans_mean = 0\n",
    "            count = 0\n",
    "            for model,model_name in zip(models,model_names):\n",
    "                print(\"------------\")\n",
    "                print(model_name)\n",
    "                model.fit(X_train,y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                model_mean = mean_squared_error(y_test,y_pred)\n",
    "                print(model_mean)\n",
    "                #print(\"------------\")\n",
    "                ans_mean += model_mean\n",
    "                count += 1\n",
    "            ans = ans_mean/count\n",
    "            fainal_ans += ans\n",
    "            fainal_count += 1\n",
    "            print(\"------------\")\n",
    "            print(\"MSE\")\n",
    "            print(ans)\n",
    "        last_ans = fainal_ans/fainal_count\n",
    "        print(\"------------\")\n",
    "        print(\"last_ans\")\n",
    "        return last_ans\n",
    "\n",
    "scb = scratch_bagi()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0251988d-b6a2-4f1f-b313-dcb1006c1379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "バギング\n",
      "------------\n",
      "Logistic Regression\n",
      "1595905365.7431376\n",
      "------------\n",
      "SVM\n",
      "5122759112.734876\n",
      "------------\n",
      "Decision Tree\n",
      "1395278361.6503043\n",
      "------------\n",
      "Random Forest\n",
      "1231856490.9752548\n",
      "------------\n",
      "MSE\n",
      "2336449832.7758927\n",
      "------------\n",
      "Logistic Regression\n",
      "1632311437.2265377\n",
      "------------\n",
      "SVM\n",
      "5183005407.582917\n",
      "------------\n",
      "Decision Tree\n",
      "1168033910.533105\n",
      "------------\n",
      "Random Forest\n",
      "993871730.2329385\n",
      "------------\n",
      "MSE\n",
      "2244305621.3938746\n",
      "------------\n",
      "Logistic Regression\n",
      "1695416427.6895301\n",
      "------------\n",
      "SVM\n",
      "5152920017.852147\n",
      "------------\n",
      "Decision Tree\n",
      "1034977415.6712328\n",
      "------------\n",
      "Random Forest\n",
      "750554424.9582678\n",
      "------------\n",
      "MSE\n",
      "2158467071.5427947\n",
      "------------\n",
      "Logistic Regression\n",
      "1574124263.847179\n",
      "------------\n",
      "SVM\n",
      "5045012747.685332\n",
      "------------\n",
      "Decision Tree\n",
      "964942289.6230024\n",
      "------------\n",
      "Random Forest\n",
      "1038411439.6805913\n",
      "------------\n",
      "MSE\n",
      "2155622685.209026\n",
      "------------\n",
      "Logistic Regression\n",
      "1676049543.2117896\n",
      "------------\n",
      "SVM\n",
      "5174913269.347317\n",
      "------------\n",
      "Decision Tree\n",
      "1378395093.0484629\n",
      "------------\n",
      "Random Forest\n",
      "868313242.2166182\n",
      "------------\n",
      "MSE\n",
      "2274417786.956047\n",
      "------------\n",
      "last_ans\n",
      "2233852599.575527\n"
     ]
    }
   ],
   "source": [
    "print(scb.bagi(X,y,X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a004dd9-2959-4526-b2a3-eb0b6a743459",
   "metadata": {},
   "source": [
    "## 【問題3】スタッキングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17905a-0dc0-455b-8ff4-812e3e238c5c",
   "metadata": {},
   "source": [
    "### スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b1cc9-af42-493b-9bd1-ab1a7527c6ec",
   "metadata": {},
   "source": [
    "## スタッキングとは"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6fb99-b0d3-4d31-9f3b-3fbe3f096549",
   "metadata": {},
   "source": [
    "### スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは $K_0=3, M_0=2$ 程度にします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bcd8cd-ebf0-49f1-8142-4193912228e0",
   "metadata": {},
   "source": [
    "《学習時》"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a314003-5197-44ca-9584-e6a3aa1fe383",
   "metadata": {},
   "source": [
    "（ステージ $0$ ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d76f75-000e-447b-9233-32bbfc233d89",
   "metadata": {},
   "source": [
    "- 訓練データを $K_0$ 個に分割する。\n",
    "- 分割した内の $(K_0 - 1)$ 個をまとめて訓練データ、残り $1$ 個を推定用データとする組み合わせが $K_0$ 個作れる。\n",
    "- あるモデルのインスタンスを $K_0$ 個用意し、異なる訓練データを使い学習する。\n",
    "- それぞれの学習済みモデルに対して、使っていない残り $1$ 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "- さらに、異なるモデルのインスタンスも $K_0$ 個用意し、同様のことを行う。モデルが $M_0$ 個あれば、 $M_0$ 個のブレンドデータが得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b6fe8-4c5d-4702-a3bc-4e819fa97826",
   "metadata": {},
   "source": [
    "（ステージ $n$ ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42eee0f-c3db-4c20-a144-24c1a72342e2",
   "metadata": {},
   "source": [
    "- ステージ $n-1$ のブレンドデータを$M_{n-1}$ 次元の特徴量を持つ訓練データと考え、 $K_n$ 個に分割する。以下同様である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51574ad-1f8b-4e70-b191-e4043116f01d",
   "metadata": {},
   "source": [
    "（ステージ $N$ ）＊最後のステージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482560e8-5100-4a0f-ac00-4ffaee3c35c4",
   "metadata": {},
   "source": [
    "- ステージ $N-1$ の $M_{N-1}$ 個のブレンドデータを$M_{N-1}$ 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97599b3-6fc4-4b5a-8c91-f5a226ee32fd",
   "metadata": {},
   "source": [
    "《推定時》"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94eaf0f-e1a0-4ae8-8031-a58875657bfa",
   "metadata": {},
   "source": [
    "（ステージ $0$ ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d7bfd-e6c4-4063-bc85-94db8a1668d2",
   "metadata": {},
   "source": [
    "- テストデータを $K_0×M_0$ 個の学習済みモデルに入力し、$K_0×M_0$ 個の推定値を得る。これを $K_0$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4e586-57fc-4fc5-b7a8-adc37c6de006",
   "metadata": {},
   "source": [
    "（ステージ $n$ ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f6b2a-2f10-4488-9fee-2f8f8202c4e7",
   "metadata": {},
   "source": [
    "- ステージ $n-1$ で得たブレンドテストを $K_n×M_n$ 個の学習済みモデルに入力し、$K_n×M_n$ 個の推定値を得る。これを $K_n$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa8398-b375-4e01-9172-69b9ec44c71f",
   "metadata": {},
   "source": [
    "（ステージ $N$ ）＊最後のステージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8515f949-38e9-4be1-b070-3121791c5159",
   "metadata": {},
   "source": [
    "ステージ $N-1$ で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36ab54c2-1032-4e66-8ebc-b2da503849f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# トレーニングデータ分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c662615-376d-4be6-8a2a-e05e390307d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"スタッキング\"\"\"\n",
    "\n",
    "class Staking():\n",
    "\n",
    "    def __init__(self,max_depth,splits,models):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.n_splits = splits\n",
    "        self.models = models\n",
    "        self.fit_models = []\n",
    "\n",
    "    def blending(self,X,y,model):\n",
    "        # blendの初期化\n",
    "        blend = np.zeros(len(X))\n",
    "        # データの分割\n",
    "        kf = KFold(n_splits=self.n_splits,shuffle=False)\n",
    "        # 分割されたインデックスでループ\n",
    "        for train_id,test_id in kf.split(X):\n",
    "            # データ分割\n",
    "            X_train = X[train_id]\n",
    "            X_test  = X[test_id]\n",
    "            y_train = y[train_id]\n",
    "            y_test  = y[test_id]\n",
    "            y_train = y_train.ravel()\n",
    "            y_test  = y_test.ravel()\n",
    "\n",
    "            # 学習\n",
    "            model.fit(X_train,y_train)\n",
    "\n",
    "            #学習ずみのモデルを保存\n",
    "            self.fit_models.append(model)\n",
    "\n",
    "            # テストデータのインデックスに予測値を格納\n",
    "            #print(model.predict(X_test))\n",
    "            blend[test_id] = model.predict(X_test)\n",
    "\n",
    "        return blend\n",
    "\n",
    "    def fit(self,X,y,depth):\n",
    "\n",
    "        # 現在の深さ\n",
    "        self.depth = depth\n",
    "        # 最大の深さまで到達していれば、その深さを学習させて処理を終了\n",
    "        if self.depth == self.max_depth:\n",
    "            # 当該深でのモデルを取得\n",
    "            self.model = self.models[self.depth]\n",
    "            # 学習\n",
    "            self.model.fit(X,y)\n",
    "            return\n",
    "\n",
    "        # 当該深さでのモデルを取得\n",
    "        models = self.models[self.depth]\n",
    "        self.blend = np.zeros([len(X),len(models)])\n",
    "\n",
    "        # この階層の全てのモデルで学習\n",
    "        for i,model in enumerate(models):\n",
    "            _blend = self.blending(X,y,model)\n",
    "            blend = _blend\n",
    "\n",
    "        # 再帰学習\n",
    "        # コンストラクタ生成の際の引数は同じ\n",
    "        self.stk = Staking(self.max_depth,self.n_splits,self.models)\n",
    "\n",
    "        # 学習実行の際は、ブレンドデータを説明変数として渡す\n",
    "        self.stk.fit(self.blend,y,depth+1)\n",
    "\n",
    "    def predict(self,X):\n",
    "        # 最大の深さの場合は最終的な予測値を出力するのみ\n",
    "        if self.depth == self.max_depth:\n",
    "            pred = self.model.predict(X)\n",
    "            return pred\n",
    "\n",
    "        else:\n",
    "            # 予測値を０で初期化\n",
    "            self.pred = np.zeros(len(X))\n",
    "            # 次の階層に渡すブレンドデータ\n",
    "            self.blend = np.zeros([len(X),len(self.models[self.depth])])\n",
    "            # この階層の学習ずみのモデルでループ\n",
    "            count = 0\n",
    "            for model in self.fit_models:\n",
    "                # 予測し、０で初期化している予測値に加算\n",
    "                self.pred += model.predict(X)\n",
    "                # 1種類のモデルの学習の終了\n",
    "                count += 1\n",
    "                if count%self.n_splits == 0:\n",
    "                    # 予測値を加算してきたので割って平均値を算出\n",
    "                    self.pred = self.pred/self.n_splits\n",
    "                    # その平均値を次の階層で使用する説明変数に格納\n",
    "                    self.blend[:,int(count/self.n_splits)-1] = self.pred\n",
    "                    # 次の種類のモデルでの予測のため、予測値は初期化しておく\n",
    "                    self.pred = np.zeros(len(X))\n",
    "            # 次の階層の予測関数\n",
    "            pred = self.stk.predict(self.blend)\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2011cd0a-611a-4049-902b-433177ccd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    0:[LinearRegression(),DecisionTreeRegressor(),RandomForestRegressor()],\n",
    "    1:LinearRegression()\n",
    "}\n",
    "\n",
    "stk = Staking(max_depth=1,splits=5,models=models)\n",
    "stk.fit(X_train,y_train,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2c2a106-ae2b-4263-b582-e67adf3b386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = stk.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5c09b4a-d26d-43fe-8bf4-93f169883078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 6599382160.934\n"
     ]
    }
   ],
   "source": [
    "# Rating\n",
    "mse = mean_squared_error(y_test,prediction)\n",
    "print('MSE : {:.3f}'.format(mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
